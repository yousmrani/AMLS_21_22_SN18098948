{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9e6916d",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73f4e19a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report,accuracy_score\n",
    "from skimage.io import imread, imshow\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Conv2D, MaxPool2D, Flatten, Activation, InputLayer, BatchNormalization, MaxPooling2D\n",
    "\n",
    "import os \n",
    "\n",
    "print(\"All imports done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf59415",
   "metadata": {},
   "source": [
    "## Define Directories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1f68302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we set the path to find the images\n",
    "\n",
    "basedir = './dataset'\n",
    "images_dir = os.path.join(basedir,'image')\n",
    "labels_filename = 'label.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61fe8b0",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "748b4dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will loop through the csv file containing the labels and create a dictionnary containing the \n",
    "# name of the image as a key and the associate label as a value\n",
    "def binary_labelling(): \n",
    "    labels_file = open(os.path.join(basedir, labels_filename), 'r')\n",
    "    lines = labels_file.readlines()\n",
    "    tumor_labels = {line.split(',')[0] : (line.split(',')[1].strip()) for line in lines[1:]}\n",
    "\n",
    "    for i in tumor_labels: \n",
    "        if tumor_labels[i] == 'no_tumor': \n",
    "            tumor_labels[i] = 0\n",
    "        else:\n",
    "            tumor_labels[i] = 1    \n",
    "    return(tumor_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "446bc2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will loop through the images and using the name of the image and the dictionnary set in the previous \n",
    "#function, will return two array containing the features and the label\n",
    "\n",
    "def extract_features_with_conv():\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    \n",
    "    labels = binary_labelling()\n",
    "\n",
    "    image_paths = [os.path.join(images_dir, l) for l in os.listdir(images_dir)]\n",
    "    if os.path.isdir(images_dir):\n",
    "        all_features = []\n",
    "        all_labels = []\n",
    "        for img_path in image_paths:\n",
    "            filename = img_path.split('/')[-1]\n",
    "            features = imread(img_path, as_gray=True)\n",
    "            all_features.append(features)\n",
    "            all_labels.append(labels[filename])\n",
    "    np_features = np.array(all_features)\n",
    "    np_features =np_features.reshape(-1,512, 512,1) # array of features needs to be changed to correspond to the  \n",
    "                                                    # input shape of CNN\n",
    "\n",
    "    np_labels = np.array(all_labels)\n",
    "\n",
    "    return np_features, np_labels\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05860f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is used to separate the dataset into a training and testing set with proportion 80% - 20% and returns\n",
    "# the associated training features, training labels, validation features and validation labels\n",
    "def get_data_with_conv(): \n",
    "\n",
    "    X, Y = extract_features_with_conv()\n",
    "    \n",
    "\n",
    "    tr_X = X[:2400]\n",
    "    tr_Y = Y[:2400]\n",
    "    te_X = X[2400:]\n",
    "    te_Y = Y[2400:]\n",
    "    \n",
    "    return tr_X, tr_Y, te_X, te_Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a237dd",
   "metadata": {},
   "source": [
    "## Create Convolutional Neural Net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd6b38c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_X, tr_Y, te_X, te_Y= get_data_with_conv() #Assign training and validation features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "227550fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_7 (Conv2D)            (None, 510, 510, 32)      320       \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 510, 510, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 255, 255, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 253, 253, 32)      9248      \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 253, 253, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 126, 126, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 124, 124, 64)      18496     \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 124, 124, 64)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 62, 62, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 246016)            0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 64)                15745088  \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 65        \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 15,773,217\n",
      "Trainable params: 15,773,217\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# The model is set to be sequential()\n",
    "model_CNN_plus = Sequential()\n",
    "#Setting the first convolution layer and giving the input shape\n",
    "model_CNN_plus.add(Conv2D(32, (3, 3), input_shape=(512, 512,1)))\n",
    "# Selecting the activation function linked to the conv layer\n",
    "model_CNN_plus.add(Activation('relu'))\n",
    "#Setting the maxpooling layer \n",
    "model_CNN_plus.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# We add another set of Conv, Activation and MaxPooling to make the CNN stronger. \n",
    "model_CNN_plus.add(Conv2D(32, (3, 3)))\n",
    "model_CNN_plus.add(Activation('relu'))\n",
    "model_CNN_plus.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# We add another set of Conv, Activation and MaxPooling to make the CNN stronger. \n",
    "model_CNN_plus.add(Conv2D(64, (3, 3)))\n",
    "model_CNN_plus.add(Activation('relu'))\n",
    "model_CNN_plus.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# This line is used to convert our model from 3d to 1d\n",
    "model_CNN_plus.add(Flatten())\n",
    "\n",
    "#Add the fully connected layer\n",
    "model_CNN_plus.add(Dense(64))\n",
    "model_CNN_plus.add(Activation('relu'))\n",
    "model_CNN_plus.add(Dropout(0.5))#We use dropout to avoid overfitting\n",
    "model_CNN_plus.add(Dense(1)) #Output layer with binary outputs\n",
    "model_CNN_plus.add(Activation('sigmoid'))\n",
    "\n",
    "model_CNN_plus.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "545ce4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For compilation we use binary cross entropy as we have 2 possible outputs.\n",
    "# We also use accuracy as metrics and adam optimsier \n",
    "\n",
    "model_CNN_plus.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer='adam')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "060bd324",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2400 samples, validate on 600 samples\n",
      "Epoch 1/20\n",
      "2400/2400 [==============================] - 1113s 464ms/step - loss: 0.5676 - accuracy: 0.8388 - val_loss: 0.3752 - val_accuracy: 0.8683\n",
      "Epoch 2/20\n",
      "2400/2400 [==============================] - 1157s 482ms/step - loss: 0.3112 - accuracy: 0.8429 - val_loss: 0.2570 - val_accuracy: 0.8683\n",
      "Epoch 3/20\n",
      "2400/2400 [==============================] - 1120s 467ms/step - loss: 0.2567 - accuracy: 0.8487 - val_loss: 0.2077 - val_accuracy: 0.8683\n",
      "Epoch 4/20\n",
      "2400/2400 [==============================] - 1138s 474ms/step - loss: 0.2040 - accuracy: 0.8529 - val_loss: 0.1960 - val_accuracy: 0.9017\n",
      "Epoch 5/20\n",
      "2400/2400 [==============================] - 1130s 471ms/step - loss: 0.1846 - accuracy: 0.9229 - val_loss: 0.1809 - val_accuracy: 0.9183\n",
      "Epoch 6/20\n",
      "2400/2400 [==============================] - 1128s 470ms/step - loss: 0.1496 - accuracy: 0.9392 - val_loss: 0.1659 - val_accuracy: 0.9250\n",
      "Epoch 7/20\n",
      "2400/2400 [==============================] - 1137s 474ms/step - loss: 0.1224 - accuracy: 0.9508 - val_loss: 0.1551 - val_accuracy: 0.9267\n",
      "Epoch 8/20\n",
      "2400/2400 [==============================] - 1123s 468ms/step - loss: 0.1103 - accuracy: 0.9521 - val_loss: 0.1465 - val_accuracy: 0.9400\n",
      "Epoch 9/20\n",
      "2400/2400 [==============================] - 1122s 467ms/step - loss: 0.0826 - accuracy: 0.9658 - val_loss: 0.1648 - val_accuracy: 0.9417\n",
      "Epoch 10/20\n",
      "2400/2400 [==============================] - 1120s 467ms/step - loss: 0.0752 - accuracy: 0.9667 - val_loss: 0.1548 - val_accuracy: 0.9450\n",
      "Epoch 11/20\n",
      "2400/2400 [==============================] - 1123s 468ms/step - loss: 0.0742 - accuracy: 0.9700 - val_loss: 0.1283 - val_accuracy: 0.9417\n",
      "Epoch 12/20\n",
      "2400/2400 [==============================] - 1082s 451ms/step - loss: 0.0539 - accuracy: 0.9837 - val_loss: 0.1669 - val_accuracy: 0.9483\n",
      "Epoch 13/20\n",
      "2400/2400 [==============================] - 1106s 461ms/step - loss: 0.0486 - accuracy: 0.9825 - val_loss: 0.2068 - val_accuracy: 0.9467\n",
      "Epoch 14/20\n",
      "2400/2400 [==============================] - 1095s 456ms/step - loss: 0.0526 - accuracy: 0.9817 - val_loss: 0.1451 - val_accuracy: 0.9467\n",
      "Epoch 15/20\n",
      "2400/2400 [==============================] - 1101s 459ms/step - loss: 0.0490 - accuracy: 0.9796 - val_loss: 0.1374 - val_accuracy: 0.9567\n",
      "Epoch 16/20\n",
      "2400/2400 [==============================] - 1101s 459ms/step - loss: 0.0391 - accuracy: 0.9871 - val_loss: 0.1453 - val_accuracy: 0.9567\n",
      "Epoch 17/20\n",
      "2400/2400 [==============================] - 1078s 449ms/step - loss: 0.0387 - accuracy: 0.9854 - val_loss: 0.1699 - val_accuracy: 0.9450\n",
      "Epoch 18/20\n",
      "2400/2400 [==============================] - 1071s 446ms/step - loss: 0.0379 - accuracy: 0.9879 - val_loss: 0.1792 - val_accuracy: 0.9583\n",
      "Epoch 19/20\n",
      "2400/2400 [==============================] - 1065s 444ms/step - loss: 0.0346 - accuracy: 0.9887 - val_loss: 0.1876 - val_accuracy: 0.9550\n",
      "Epoch 20/20\n",
      "2400/2400 [==============================] - 1047s 436ms/step - loss: 0.0296 - accuracy: 0.9892 - val_loss: 0.2006 - val_accuracy: 0.9550\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f9880f7fb10>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training the model\n",
    "model_CNN_plus.fit(tr_X, tr_Y,batch_size = 64, epochs = 20 ,validation_data=(te_X, te_Y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2537938c",
   "metadata": {},
   "source": [
    "## Save model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a2cebc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model takes 10 hours to run...better to save it\n",
    "model_CNN_plus.save('CNN_model_binary')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfc8b4a",
   "metadata": {},
   "source": [
    "## Load Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e42febc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-05 12:33:05.259356: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-01-05 12:33:05.260202: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 8. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "# Useful if model is saved before\n",
    "model = tf.keras.models.load_model('CNN_model_binary')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7ef32c",
   "metadata": {},
   "source": [
    "## Evaluate model on testing dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8426fa18",
   "metadata": {},
   "source": [
    "### Preprocessing of testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e75d7ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preparing testing dataset by modifying the function used earlier\n",
    "\n",
    "testdir = './test'\n",
    "test_images_dir = os.path.join(testdir,'image')\n",
    "test_labels_filename = 'label.csv'\n",
    "\n",
    "def binary_labelling_testset(): \n",
    "    labels_file = open(os.path.join(testdir, test_labels_filename), 'r')\n",
    "    lines = labels_file.readlines()\n",
    "    tumor_labels = {line.split(',')[0] : (line.split(',')[1].strip()) for line in lines[1:]}\n",
    "\n",
    "    for i in tumor_labels: \n",
    "        if tumor_labels[i] == 'no_tumor': \n",
    "            tumor_labels[i] = 0\n",
    "        else:\n",
    "            tumor_labels[i] = 1    \n",
    "    return(tumor_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1066871e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_with_conv_testset():\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    \n",
    "    labels = binary_labelling_testset()\n",
    "\n",
    "    image_paths = [os.path.join(test_images_dir, l) for l in os.listdir(test_images_dir)]\n",
    "    if os.path.isdir(images_dir):\n",
    "        all_features = []\n",
    "        all_labels = []\n",
    "        for img_path in image_paths:\n",
    "            filename = img_path.split('/')[-1]\n",
    "            features = imread(img_path, as_gray=True)\n",
    "            all_features.append(features)\n",
    "            all_labels.append(labels[filename])\n",
    "    np_features = np.array(all_features)\n",
    "    np_features =np_features.reshape(-1,512, 512,1)\n",
    "\n",
    "    np_labels = np.array(all_labels)\n",
    "\n",
    "    return np_features, np_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78867349",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9580c187",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We use the functions defined earlier to assign the testing values\n",
    "testset_X, testset_Y = extract_features_with_conv_testset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48f286e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/1 [================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 6s 32ms/sample - loss: 0.0446 - accuracy: 0.9450\n",
      "test loss, test acc: [0.0892625638356003, 0.945]\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(testset_X, testset_Y)\n",
    "\n",
    "print(\"test loss, test acc:\", results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dcf00d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "# We use the model on testing set\n",
    "y_pred = model.predict_classes(testset_X, verbose = 0)\n",
    "\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "975e1d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   no tumour       0.86      0.84      0.85        37\n",
      "      tumour       0.96      0.97      0.97       163\n",
      "\n",
      "    accuracy                           0.94       200\n",
      "   macro avg       0.91      0.90      0.91       200\n",
      "weighted avg       0.94      0.94      0.94       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "c_report_testing = classification_report(testset_Y, y_pred, target_names = [\"no tumour\", \"tumour\"])\n",
    "print(c_report_testing)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
